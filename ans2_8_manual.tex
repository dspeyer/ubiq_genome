To reduce the number of errors in the reads, we could try several methods:

First, we could have replicated the DNA with PCR using random primers to have more copies of
fewer fragments. By sequencing multiple fragments of the same sequence we could have some redundancy and 
use that to cross-check our data.  Since this involves the materials preperation, it is too late to do it now.

Something we can do is look at the quality scores and only pay attention to high ones.
This could lead to throwing out poor data, and improving the overall accuracy
of the reads. Though this sounds like a solid strategy in theory, it should be
noted that in the last hackathon, we found that the quality scores were not
very good. Also, if there were certain sequences that are specifically difficult to sequence
for whatever reason, this method would not include them, introducing selection bias.

Also, we can improve accuracy by only believing polymorphisms that are known 
to be in $>$1\% of the population. Otherwise, we must believe it is a sequencing error, and
not a variation. 
